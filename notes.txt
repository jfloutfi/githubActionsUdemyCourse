workflow automation service by github
CI/CD

continous integration -> integrating new changes to existing code base
continous delivery/deployment

new syntax: 
git branch -> allows you to create branches instead of checkout -b
git swicth to swicth branches
git switch -c -> create and switch

git remote add origin url
origin is a name, this can be written as: git remote add someOnlineRepo url
the term origin is just a name (alias) for a remote repository URL.

git remote set-url https://jfloutfi@github.com/jfloutfi/someRepo.git
setting the remote URL like this tells github that when interacting with this repo, assume user jfloutfi
then: git push origin main -> will ask us for a password. This can then be created in settings -> Developer settings -> Personal access toke
make sure to check "repo"
generate the token and copy it
once use locally as a password, the system will store it for future use
use --set-upstream to connect the local branch with its remote conterpart branch

git remote and git remote get-url origin will show us the alias and the url or the remote repo

Pull requests can be created accross forks
once you fork a repo and you push some changes to another branch,
you can then, from the page of the orignal repo, created a pull request from the branch on your fork to the original repo

GitHub Actions
--------------
We can have multiple workflows
workflows can run un parallel

each workflow can have multiple jobs
jobs can run in parallel
jobs can be conditional, for example only run if the previous job succeeds or fails

each job can have multiple steps
steps are sequential
steps can be conditional


The first line must be: "name"
name: my workflow

on: key reserved to define the event to when to trigger this workflow, workflow_dispatch value make sure we can manually trigger this workflow

jobs: below you define the jobs, yml works with indentations
jobs:
    fist-job: // this is the job name, it is not a reseved keyword
        runs-on: ubuntu-latest => search for these online, you will find the list of available runners in github documentation https://github.com/actions/runner-images
        steps: // below we define the steps, each starting with a "-"
            - name: Print greeting
              run: command to run

Since we are using a github server, we need to get the code from the repo before running things
The github server does not have access to our code automatically
in this case, we need a step with an action
run: => shell commands
uses: (for actions) a custom application that performs a frequent complex repeated task
there are official actions but the github team and one contributed by the comunity
There is an actions marketplace, all actions there are actually free 
Use the "checkout" action to fetch your code
actions/checkout@v5 => version is optional, but it is better to lock it in in case of future breaking changes
the "with" keyword, belongs to the "use" keyword, and can be used to set config options to the action

When it comes the runners, check the github documentation, most runners have preinstalled software
like ubunut-latest already has nodejs installed

in case you get a permission error, you will need to generate a new token from settings -> developer settings
that allows updating workflows

working dir can be set per step:
working-directory: my-folder (in case we want to cd into a certain dir)

or as a default for all steps at job level
runs-on: ubuntu-latest
defaults:
  run:
    working-directory: my-folder

the default can be specified at workflow level and applies to all jobs

by default, jobs run in parallel
in order to run them sequentially, we need to specify that the 2nd step needs the 1st
the keyword used here is "needs", the value is the 1st job's name
the value can also be an arryay of job names, to make the step wait for multiple jobs to finish 

we can of course have multiple triggers, for example:
on: [push, workflow_dispatch]

Context Objects:
----------------
run: echo "${{ toJSON(github) }}"
we use ${{ ... }} => contains an expression
github context object, there are many others that you can check in the documentation
https://docs.github.com/en/actions/concepts/workflows-and-actions/expressions
https://docs.github.com/en/actions/reference/workflows-and-actions/contexts
context names are reserved keywords

toJSON => function provided by github to transform the object into printable data

Control workflow with types and filters
---------------------------------------
example, no on every push I want to trigger a workflow

each event (push, workflow_dispatch... etc) has activity types
for example:

instead of "on: push", you use yml indented structure style like steps, for example:
on:
  pull_request:
    types:
      - opened
  workflow_dispatch: we have to add the rest of the events like this even if we are not adding any filters to them

defaults:
  run:
    working-directory: 01-Starting-Project


as for filters, an example when pushing tousing branches:
on:
  push:
    branches:
      - main
      - 'dev-*' => this is a regex for branches that start with 'dev-'
      - 'feat/**' => the double '**' allows for further slashes

we can also have negative filters: branches-ignore for example (check docs for this)
another important filter is the "path" filter, it allows triggering workflows based on which files changes

when it comes to pull requests, it has something special
this is related to opening pull request from forked repos:
for example, people can fork a repo, work on the code themselves,
then open a PR to the original repo from that forked repo instead
of working on a branch for example
creating a PR from a forked repo that is targetting the original repo
qualifies for triggering the workflow
however, the item in "actions", shows up with the exclamation mark
that is because in this case, it requires approval to run
this is usefull to protect the original repo's workflow quota (and money) and spam them with workflow runs
Only the 1st PR in this case will require approval
PS: if the person was already added as a collaborator from the repo settings
then their workflow will run even if comming from a fork since you have added them yourself

workflow runs can be canceled or skipped:
-----------------------------------------
on failure, if a job/step fail, it stops the workflow
this can be customized, my making jobs dependend on independant from each other... etc
workflows can also be canceled manually

github actions provide built in ways to skip workflows
this can be done by finishing commit message with certain substrings
for example "blabla blablba [skip ci]", there are strings you can use, check them in the documentation

THESE ARE SPECIFIC TO PULL AND PUSH EVENTS

Workflows that would otherwise be triggered using on: push or on: pull_request won't be triggered if you add any of the following strings to the commit message in a push, or the HEAD commit of a pull request:
[skip ci]
[ci skip]
[no ci]
[skip actions]
[actions skip]

job artifcats and outputs
-------------------------
This the ourput generated by a job
we can download them manually
we can download them automatically and use them in other jobs

these can also be log files, but mostly, it is stuff that we wanna use in other jobs

NOTE: when the workflow ends, the runner machine is shutdown and its data lost

- name: upload artificat
  uses: actions/upload-artificat@v3
  with:
    name: dist-files => we can name this anything we want
    path: we can have 1 or multiple paths, we use the pipe | when adding multiple paths

Also, the "!" can be used to exlude files

the workdir is alwasy our repo, or the one we specify using: "working-directory: starting-project-01"

IMPORTANT 
---------
when the base dir is not the working dir, you will need to checkout in every job
each job is a new canavs and without checking out, the repo will not be there
OR OR OR, the work dir can be reset at job or set level as such
=> since you set defaults.run.working-directory at the workflow level, every job inherits it. To make one job run from the repo root, just override the default for that job and point it back to . (or ${{ github.workspace }}).

# Override the workflow default just for this job:
defaults:
  run:
    working-directory: .  # or: ${{ github.workspace }}

- name: Run from repo root only here
    run: make package
    working-directory: .               # overrides just this step

Also, upload-artifact will look for files and folders in the base directory,
so you will have to include the workdir in the path:
"./starting-project-01-02/dist" instead of "./dist"

- name: upload artificat
  uses: actions/upload-artifact@v4
  with:
    name: dist-files => it will be uploaded as .zip file
    # the output of "run build" in stored in the ./dist folder
    # based the build command create by the user
    path: |
      ./starting-project-01-02/dist
      ./starting-project-01-02/package.json

As for downloading the built artificate:
- name: get build artifacts
  uses: actions/download-artifact@v4
  with:
    name: dist-files => sepcify the name of the artificat

the artificat will be unzip automatically
the files will be found in the base directory, similarly to upload

output values in Github actions are different from artificats. For example:
"outputs" is added before teach of the steps are defined as part of "steps" config fields

outputs:
  script-file: ${{ steps.publish_id.outputs.script-file }} # steps is a special context variable, we can access step related data using it, the best way would be to give that step an "id". Referencing the output here set this to be the output of the overall job. "script-file:" is not a reserved keyword, it is up to us to name it. We can also add multiple output
steps:
  - name: Get code
    uses: actions/checkout@v4
  - name: Install dependencies
    run: npm ci
  - name: Build website
    run: npm run build
  - name: publish js file name
    id: publish_id # the step id
    run: find dist/assets/*.js -type f -execdir echo 'script-file={}' >> $GITHUB_OUTPUT ';' # {} special place holder, will hold the name of the .js files. We stream the output ">>" to the special env variable variable: $GITHUB_OUTPUT


run: echo "${{ needs.build.outputs.script-file }}"
we could have use the jobs context variable for this, but needs contains
the data of the jobs that have been defined as a dependency for this current job

IMPORTANT: Caching dependencies
-------------------------------
every job runs on its own runner/machine, but
we can cache dependencies accross jobs and Workflows
which help reduce the total run time

- name: cache dependencies
  uses: actions/cache@v4
  with:
    path: ~/.npm # npm builds a cache and other such tools build a cache that they use when deps are being re-installed, in this case, we want to reuse this across jobs since jobs use different machines
    key: deps-node-modules-${{ hashFiles('**/package-lock.json') }} # used to get the cached data
    # hashFiles() is a function that produces a unique has based on a file we provide. When the file content changes, the hash changes
    # for node projects, one of the best files to use is the .lock file, as long as the deps do not change, the file will remain the same
    # for cache keys, it is better to use dynamic names as this helps when it comes to needing to discard this cache

the cache action will also look at the foder after the jobe ends
and if there are changes, the cached data will get updated

Github provides very good offcial examples for many languages

NOTE: the cache uses 1 central cache across jobs and Workflows

in order to use the cache in a another step, you include the same step with the cache action, path and key

Environment Variables
---------------------
They can be defind at workflow or job level (scope)
IMPORTANT: storing passwords ad user names in clear text in workflow file is not IDEAL
in fact, it is a security risk
When we want to use an env variable, in the case of a run instruction (bash with linux), we simply use "$VAR_NAME"
we can also switch the "shell" by adding this keyword to the step

we can also use the "env" context object to access these Variables
for example, ${{ env.VAR_NAME }}

github provides some default env variables, they can be found here:
https://docs.github.com/en/actions/how-tos/write-workflows/choose-what-workflows-do/use-variables#default-environment-variables

Secrets
-------
Go to:
your repo -> settings -> Secrets and variables -> Actions
you will be able to set your sercrets there. Once the secret is set, is can not be viewed anymore
Also, repo variables can be added there.
-> either at repo level, or per environment (these envs can be also create there in order to group and organise secrets and vars)

PS: the secret can be updated (set new value), but can not be seen

in order to use these secrts, with the "secrets" context object
${{ secrets.VAR_NAME }}

even if you try to print such a value, or read it, store it in an env variable then try to print it
github will recognize this and hide it

Environments
------------
these envs are configure from the repo settings and referenced in workflows as such:
per step, you define the "environment" you are using
and you use "secrets" and "vars" to access these secrets and env variables

test-3:
  environment: testing
  runs-on: ubuntu-latest
  steps:
    - name: test vars and secrets
      run: |
        echo "${{ secrets.MY_ENV_SEC }}"
        echo "${{ vars.MY_ENV_VAR }}"
test-4:
  environment: prod
  runs-on: ubuntu-latest
  steps:
    - name: test vars and secrets
      run: |
        echo "${{ secrets.MY_ENV_SEC }}"
        echo "${{ vars.MY_ENV_VAR }}"

Here we can also add some protections (settings on github):
for example, we can sepcify that a workflow requires a reviewrs approval to get executed
these rules can also be limited to certain branches for example, this is different from the "branches" key set in the workflow files, this concerns these protection rules

Conditional Execution
---------------------
can be added at job or step level
we can also continue even if there is an error
expressions can be used to add custom conditions

for example, in a step, we run some tests
if they all pass, then we might not be interested in
uploading the test report... but if it fails, that report can be uploaded for further inspection

in github action, if a step fails, then the entire job to which that step belongs to
is canceled BUT we might be interested in running the next steps anyway

also, if a job fails, the jobs that depend on it will be canceled as well

# for "if", we can omit the use of ${{}} as for this key, the value is typically and expression
if: steps.run-tests.outcome
.outcome: before continue-on-error is applied
vs
.conclusion: after continue-on-error is applied

the results of the 2 above are strings that would need to be compared using '==' for example

# for "if", we can omit the use of ${{}} as for this key, the value is typically and expression
# this as is will still not execute, we need 1 more function
# if: steps.run-tests.outcome == 'failure'
if: failure() && steps.run-tests.outcome == 'failure'

we have 4 of these functions:
- failure() => returns true if any of the previous steps fails
- success() => returns true if none of the previous steps fails
- always() => returns true always even when canceled
- canceled() => returns true if a "workflow" has has been canceled

"if" and these 4 functions can also be used a JOB level
so even if a job fails, we can have the pipeline continue
For example, adding the bellow as a final job
as is, it will run in parallel at the start and it will not behave as we expect it to
we need the "needs" keyword
report:
  runs-on: ubuntu-latest
  if: failure()
  run: |
    echo something when wrong
    echo "${{ toJSON(github) }}"

For example, we can store the node-modules folder instead of the .npm one
this folder already has the dependencies installed vs .npm containing the binaries
in this case, when Referencing the cache, we want to check if the we have a cache hit,
else we need to install the modules.
The "cache" itself has its own outout, this can be found in its documentation: https://github.com/actions/cache
it has an ouput variable called "cache-hit" that gets set to 'true' or 'false'
If want to use it with a if statement, we can use it as such:
if: steps.cache_step_id.outpus.cache-hits == 'true' or != 'true'
cache_step_id => this you specify use "id" in the step when you handle cache
i.e if condition can be used for other things and not just fail/success and similar statuses

continue-on-error:
------------------
- name: Test code
  id: run-tests # set the id to whatever you want
  continue-on-error: true # we can gibe this an expression as well instead of hardcoding "true"
  run: npm run test
- name: Upload test report
  # for "if", we can omit the use of ${{}} as for this key, the value is typically and expression
  # this as is will still not execute, we need 1 more function
  # if: steps.run-tests.outcome == 'failure'
  # if: failure() && steps.run-tests.outcome == 'failure'
  # here we can use another method: "continue-on-error"
  # this is set on the step before this so that the execution contines even if the step fails

BUT BUT BUT there is a big difference:
if "continue-on-error: true" is set, even if the step fails, the job at the end will be marked as "succeeded"
so jobs that depend on it will continue

this is where .conclusion (after continue-on-error is set to true) and .outcome (before continue-on-error is set to true) come into play as mentined before

continue-on-error can also be set on job level, in this case,
if the job fails, it will be marked as failed (unlike with steps where the job ends being marked as passed, but the step in the logs shows up as failed), but other related jobs will coninue

Matrix Strategies:
------------------
the idea of a Matrix is to run the job with different configurations
for example:
jobs:
  build:
    strategy:
      matrix:
        node-version: [12, 14, 16]
        operating-system: [ubuntu-latest, windows-latest]
        # other values can be used here, we can name the keys whatever we want
        # though there are some reserved keys

then, when we want to use it, we use the "matrix" context object, for example:
runs-on: ${{ matrix.operating-system }}

in the actions tab on github, we would see: runing 0/4 jobs for example
when using a matrix, if one of the jobs fail, the others are canceled
in this case, we use "continue-on-error" on "job" level and set it to true

for matrix, we can use the "include" key, this one is one fo the reserved keys
this allows is to create specific combinations, for example
include:
  - node-version: 18
    operating-system: ubuntu-latest
    # we can also add news keys that were not mentioned above
  - node-version: 20
    operating-system: windows-latest

as such, version 18 will only be ran on ubuntu-latest only
and version 20 will only be ran on windows-latest only

we can also use the "exclude" reserved keyword
exclude:
  - node-version: 12
    operating-system: windows-latest

Reusable Workflows
------------------
!!! IMPORTANT: not to be confused with "Reusable Actions"

to be able to call a reusable workflow:
we use a special trigger for the "on" keyword
on: workflow_call # when called from other workflows
and we need to use it, we use the "uses"  keyword and reference the workflow from the root dir
uses: ./.github/workflows/reusable.yml

so the job would be defined as:
use-reusable-deploy:
  needs: build
  uses: ./.github/workflows/reusable.yml

and the reusable workflow would be:
name: reusable deploy
on: workflow_call # when called from other workflows
jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - name: output-information
        run: echo "deploying..."

reusable workflows can be though of as functions
for best practice, it is best not to harcode certain names, like for example, when downloading
an artifact by name
here, we can think of these as function args, example below
here, the "inputs" context variable is available for use

name: reusable deploy
on:
  workflow_call: # when called from other workflows
    inputs:
      artifact-name: # key names for inputs are up to us to choose
        descritpion: the name of the artifact we want to download
        required: true # or false => false, mean the workflow will run even if no input is provided, else it will fail
        default: dist # default value in case the user provided none, this works well with "required: false" as the workflow has a name to look up if the user did not provide any
        type: string
jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - name: get code
        uses: actions/download-artifact@v4
        with:
          name: ${{ inputs.artifact-name }}
      - name: output-information
        run: echo "deploying..."

we can also pass secrets using the workflow alongside inputs using the key "secrets"
these can be referenced using the "secrets" context variable

Finally, the reusable workflow can produce outputs and pass it back to the calling workflow
we can think of them as the return values of functions.
So, we need to create a chain of outputs
output from the step to the job
output from the job to the workflow
output from the workflow to the calling workflow

on:
  workflow_call: # when called from other workflows
    inputs:
      artifact-name: # key names for inputs are up to us to choose
        descritpion: the name of the artifact we want to download
        required: true # or false => false, mean the workflow will run even if no input is provided, else it will fail
        default: dist # default value in case the user provided none, this works well with "required: false" as the workflow has a name to look up if the user did not provide any
        type: string
    outputs:
      result:
        description: the resulting value of the workflow
        # workflow output set to the value of the job output
        value: ${{ jobs.deploy.outputs.outcome }}

jobs:
  deploy:
    outputs:
      # job output set to the value of the step output
      outcome: ${{ steps.set-result.outputs.step-result }}
    runs-on: ubuntu-latest
    steps:
      - name: get code
        uses: actions/download-artifact@v4
        with:
          name: ${{ inputs.artifact-name }}
      - name: list files
        run: ls
      - name: output-information
        run: echo "deploying..."
      - name: set result output
        # set output
        run: echo "step-result=success" >> $GITHUB_OUTPUT

and in the calling workflow:

print-deploy-result:
  needs: use-reusable-deploy
  runs-on: ubuntu-latest
  steps:
    - name: print reusable workflow output
      run: echo ${{ needs.use-reusable-deploy.outputs.result }}

we use the "needs" context variable in this case to reference the previous job
since it is needed by this one

Github Action and Containers
----------------------------
add steps to container is simple
at job level, we use the key: "container"
the value is the image we want to use
but for this, we have to use images that are pubically avaiable, like the ones on dockerhub
we can publish our own, but they have to be public so that github action can fetch them
we can also add nested key in case we want to set more configurations:

container:
  image: node:16
  env: ... etc

in this case, the machine specified in "runs-on" will host the image

the steps running in the image will not have direct access to the machine nor the tools available on it
but, from this point on, the rest of the steps will run inside the container
in the logs output on github, you will see the Initialize Container steps
and in theory, anything you can do via github action on the main machine can be done in a container (accessing secrests, using actions... etc)

this helps a lot when we need full control of the env when we are running our job

We can also use container in conjunction to github action to spin up service Containers
For example, when running tests, our code might need to interact with a database that is not the production database
so we can use service container to host a test database

for this, we use the "services" key,
this is set at job level and we can have multiple services per job, for example:
services:
  mongodb: # the label or name can be anything we want
    image: mongo #services always run in containers, so we always need container
    ports:
      - 123:456 # map external container port to internal one to reach the service
    env: # this might depend on the image can be found in the image documentation
      SOME_VAR_1: bla
      SOME_VAR_2: blo

github actions creates a network and we can use the service name or lable to connect to it
on the condition that the job is also running in a container

in case the job in running on the main machine, then we use the localhost IP address with a port
it is the exposed port of the service container that maps to the port of the service running inside it

Custom Actions
==============
Different types of actions
- javascript actions
  execute js sctipts, use any lib you want
- docker actions
  a containerized action, basically an app that you put in a container and make it run (could be any kind of script in any language you prefer)
- composite actions
  you combine multiple workflow steps in a single action, greate for combining multiple steps that get reused in the same or multiple projects

In order to share custom actions across repos, create them in a standalone repo, else they will only be available in the repo they are shared.

IMPORTANT: when we are defining a action as part of the repo itself, we can not include
checking out the code in it, as we need to checkout the project's code 1st which includes the action
to be able to use it

composite action
----------------
When creating a custom composite action, if we have a "run" with commands in it, we must also specify the shell to be used, for example:
run: npm ci
shell: bash

a full example:
name: get & cache dependencies
description: get deps and cache them
runs:
  using: "composite" # tells githib action that this is a composite action
  steps:
    - name: Cache dependencies
      id: cache
      uses: actions/cache@v4
      with:
        path: ./starting-project-01-06/node_modules # in a normal project, node_modules dir would be at root dir
        key: deps-node-modules-${{ hashFiles('**/package-lock.json') }}
    - name: Install dependencies
      working-directory: starting-project-01-06 # done for this leanring repo only
      if: steps.cache.outputs.cache-hit != 'true'
      run: npm ci
      shell: bash

inputs and outputs
------------------
we use the "inputs" key
inputs:
  caching: key up to the user to sepcify
    description: reserved key, must be added and takes a string value

accessing it later one:
inputs.caching == 'true' for example
we pass these inputs using the "with" keyword in the calling workflow

for outputs:
outputs:
  used-cache:
    description: if cached was used or not
    value: ${{ steps.install.used-cache }}

note: 
requires adding an id to the step to be able to reference it
needs to write an output in the commands in the step
echo 'used-cache=true' >> $GITHUB_OUTPUT

and to output it:
- name: Load and Cache dependencies
  id: cache-step
  uses: ./.github/actions/cached-deps # ./ i.e start at the root folder of the project
  with:
    caching: "false"
  # no need to point to the file itself, github will automatically look for an action.yml file
- name: action output
  run: echo "cache used? ${{ steps.cache-step.outputs.used-cache }}"
